{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 0.3 \u2014 Baseline Models\nTrain simple baseline classifiers to predict trade outcome (win/loss).",
   "id": "md#0.3\u2014"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import pandas as pd\nimport numpy as np\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import (accuracy_score, classification_report,\n                              confusion_matrix, roc_auc_score)\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nBASE = '.'\nTRADERS = ['calm_trader', 'loss_averse_trader', 'overtrader', 'revenge_trader']\nLABELS  = {'calm_trader':'Calm','loss_averse_trader':'Loss Averse',\n           'overtrader':'Overtrader','revenge_trader':'Revenge'}\n",
   "outputs": [],
   "execution_count": null,
   "id": "cimportp"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load Train/Val/Test Splits",
   "id": "md##Load"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "splits = {}\nfor t in TRADERS:\n    proc = f'{BASE}/{t}/data/processed'\n    splits[t] = {\n        'train': pd.read_csv(f'{proc}/{t}_train.csv'),\n        'val':   pd.read_csv(f'{proc}/{t}_val.csv'),\n        'test':  pd.read_csv(f'{proc}/{t}_test.csv'),\n    }\n    for s, df in splits[t].items():\n        print(f\"{t} {s}: {len(df)} rows, win_rate={df['win'].mean():.3f}\")\n",
   "outputs": [],
   "execution_count": null,
   "id": "csplits="
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Feature Selection",
   "id": "md##Featu"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "FEATURES = ['quantity', 'entry_price', 'exit_price',\n           'asset_encoded', 'side_encoded',\n           'year', 'month', 'day', 'hour', 'minute',\n           'price_change', 'price_change_pct']\nTARGET = 'win'\n\ndef get_XY(df):\n    df = df.dropna(subset=FEATURES + [TARGET])\n    return df[FEATURES], df[TARGET]\n",
   "outputs": [],
   "execution_count": null,
   "id": "cFEATURES"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Define Models",
   "id": "md##Defin"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "MODELS = {\n    'Dummy (Majority)':   DummyClassifier(strategy='most_frequent'),\n    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n    'Decision Tree':       DecisionTreeClassifier(max_depth=5, random_state=42),\n    'Random Forest':       RandomForestClassifier(n_estimators=100, max_depth=6, random_state=42, n_jobs=-1),\n}\n",
   "outputs": [],
   "execution_count": null,
   "id": "cMODELS="
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Train & Evaluate on Validation Set",
   "id": "md##Train"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "results = {}\n\nfor t in TRADERS:\n    X_train, y_train = get_XY(splits[t]['train'])\n    X_val,   y_val   = get_XY(splits[t]['val'])\n\n    scaler = StandardScaler()\n    X_train_s = scaler.fit_transform(X_train)\n    X_val_s   = scaler.transform(X_val)\n\n    results[t] = {}\n    for name, model in MODELS.items():\n        model.fit(X_train_s, y_train)\n        preds = model.predict(X_val_s)\n        proba = model.predict_proba(X_val_s)[:, 1] if hasattr(model, 'predict_proba') else preds\n        acc   = accuracy_score(y_val, preds)\n        auc   = roc_auc_score(y_val, proba) if len(set(y_val)) > 1 else 0.5\n        results[t][name] = {'acc': acc, 'auc': auc, 'model': model, 'scaler': scaler}\n        print(f\"{LABELS[t]} | {name:25s} \u2192 Acc: {acc:.4f}  AUC: {auc:.4f}\")\n    print()\n",
   "outputs": [],
   "execution_count": null,
   "id": "cresults"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Results Heatmap",
   "id": "md##Resul"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "model_names = list(MODELS.keys())\ntrader_names = [LABELS[t] for t in TRADERS]\n\nacc_matrix = np.array([[results[t][m]['acc'] for m in model_names] for t in TRADERS])\nauc_matrix = np.array([[results[t][m]['auc'] for m in model_names] for t in TRADERS])\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\nfor ax, mat, title in zip(axes, [acc_matrix, auc_matrix], ['Accuracy', 'ROC-AUC']):\n    im = ax.imshow(mat, cmap='RdYlGn', vmin=0.4, vmax=0.7)\n    ax.set_xticks(range(len(model_names))); ax.set_xticklabels(model_names, rotation=20, ha='right', fontsize=9)\n    ax.set_yticks(range(len(trader_names))); ax.set_yticklabels(trader_names)\n    ax.set_title(f'Validation {title}')\n    for i in range(len(trader_names)):\n        for j in range(len(model_names)):\n            ax.text(j, i, f'{mat[i,j]:.3f}', ha='center', va='center', fontsize=9, fontweight='bold')\n    plt.colorbar(im, ax=ax)\nplt.tight_layout(); plt.show()\n",
   "outputs": [],
   "execution_count": null,
   "id": "cmodel_na"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Best Model \u2014 Test Set Evaluation",
   "id": "md##Best"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\" * 60)\nfor t in TRADERS:\n    best_name = max(results[t], key=lambda m: results[t][m]['auc'])\n    best = results[t][best_name]\n\n    X_test, y_test = get_XY(splits[t]['test'])\n    X_test_s = best['scaler'].transform(X_test)\n    preds = best['model'].predict(X_test_s)\n\n    print(f\"\\n{LABELS[t]} \u2014 Best Model: {best_name}\")\n    print(classification_report(y_test, preds, target_names=['Loss','Win']))\n    print(\"=\" * 60)\n",
   "outputs": [],
   "execution_count": null,
   "id": "cprint(\"="
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Confusion Matrices",
   "id": "md##Confu"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\nfor ax, t in zip(axes.flat, TRADERS):\n    best_name = max(results[t], key=lambda m: results[t][m]['auc'])\n    best = results[t][best_name]\n    X_test, y_test = get_XY(splits[t]['test'])\n    X_test_s = best['scaler'].transform(X_test)\n    preds = best['model'].predict(X_test_s)\n    cm = confusion_matrix(y_test, preds)\n    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues',\n                xticklabels=['Loss','Win'], yticklabels=['Loss','Win'])\n    ax.set_title(f\"{LABELS[t]}\\n({best_name})\")\n    ax.set_xlabel('Predicted'); ax.set_ylabel('Actual')\nplt.tight_layout(); plt.show()\n",
   "outputs": [],
   "execution_count": null,
   "id": "cfig,axe"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Feature Importance (Random Forest)",
   "id": "md##Featu"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\nfor ax, t in zip(axes.flat, TRADERS):\n    rf = results[t]['Random Forest']['model']\n    imp = pd.Series(rf.feature_importances_, index=FEATURES).sort_values(ascending=True)\n    imp.plot(kind='barh', ax=ax, color='#2196F3', alpha=0.85)\n    ax.set_title(f'{LABELS[t]} \u2014 RF Feature Importances')\n    ax.set_xlabel('Importance'); ax.grid(axis='x', alpha=0.3)\nplt.tight_layout(); plt.show()\n",
   "outputs": [],
   "execution_count": null,
   "id": "cfig,axe"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n> These baselines establish a performance floor. All models will be compared against these benchmarks in subsequent notebooks.",
   "id": "md##Summa"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "summary = []\nfor t in TRADERS:\n    for name in MODELS:\n        r = results[t][name]\n        summary.append({'Trader': LABELS[t], 'Model': name,\n                        'Val Acc': round(r['acc'], 4), 'Val AUC': round(r['auc'], 4)})\n\ndf_summary = pd.DataFrame(summary)\nprint(df_summary.to_string(index=False))\n",
   "outputs": [],
   "execution_count": null,
   "id": "csummary"
  }
 ]
}